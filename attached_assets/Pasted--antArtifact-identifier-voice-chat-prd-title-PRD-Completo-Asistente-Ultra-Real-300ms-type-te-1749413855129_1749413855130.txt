<antArtifact identifier="voice_chat_prd" title="PRD Completo: Asistente Ultra-Real <300ms"  type="text/markdown" version_uuid="72a9639d-9c07-4657-81bc-ed47a0e1d951">
# PRD Completo: Asistente de Voz Ultra-Real <300ms

## 1. Resumen Ejecutivo

### 1.1 Visi√≥n del Producto
Crear el asistente de voz m√°s natural y r√°pido posible usando voz clonada real, con latencia menor a 300ms, interrupciones fluidas, detecci√≥n autom√°tica de idiomas y respuestas con modismos naturales.

### 1.2 Objetivos Cr√≠ticos
- **Latencia Ultra-Baja**: <300ms end-to-end medidos desde fin de speech hasta inicio de respuesta
- **Voz Ultra-Real**: Voz clonada indistinguible del usuario real con modismos naturales
- **Interrupciones Naturales**: Sistema que maneja cortes de conversaci√≥n como humanos reales
- **Multiling√ºe Inteligente**: Detecci√≥n autom√°tica y cambio din√°mico entre idiomas
- **Presupuesto Controlado**: <$50/mes total para uso personal/profesional

## 2. Especificaciones T√©cnicas Detalladas

### 2.1 Arquitectura de Ultra-Baja Latencia

#### Stack de Tecnolog√≠as Optimizadas
```yaml
Comunicaci√≥n:
  Control Channel: WebSocket (comandos, transcripts, estado)
  Audio Channel: WebSocket Binary (chunks de 20ms)
  Protocolo: Dual WebSocket para separar control y audio
  
Audio Processing:
  Capture: Web Audio API + AudioWorklets
  Sample Rate: 16kHz (optimizado para APIs)
  Chunk Size: 20ms (320 bytes) para ultra-responsividad
  Format: PCM 16-bit mono, little-endian
  Buffer: Circular buffer 500ms con overflow protection

VAD (Voice Activity Detection):
  Method: Hybrid WebRTC VAD + energy-based detection
  Sensitivity: Adaptable per usuario (aggressive/balanced/conservative)
  Turn Detection: 300ms silence threshold (configurable por idioma)
  Interruption: Real-time monitoring durante TTS playback
```

#### Flujo de Latencia Optimizado (Target <300ms)
```
Component Breakdown:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Componente          ‚îÇ Target (ms) ‚îÇ M√°ximo (ms) ‚îÇ Optimizaci√≥n‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Audio Capture       ‚îÇ 20          ‚îÇ 50          ‚îÇ AudioWorklet‚îÇ
‚îÇ Network Upload      ‚îÇ 30          ‚îÇ 80          ‚îÇ WebSocket   ‚îÇ
‚îÇ STT Processing      ‚îÇ 80          ‚îÇ 150         ‚îÇ Deepgram    ‚îÇ
‚îÇ LLM Inference       ‚îÇ 100         ‚îÇ 150         ‚îÇ Groq Stream ‚îÇ
‚îÇ TTS Synthesis       ‚îÇ 300         ‚îÇ 500         ‚îÇ ElevenLabs  ‚îÇ
‚îÇ Network Download    ‚îÇ 30          ‚îÇ 80          ‚îÇ Streaming   ‚îÇ
‚îÇ Audio Playback      ‚îÇ 20          ‚îÇ 50          ‚îÇ Direct play ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ TOTAL END-TO-END    ‚îÇ 580         ‚îÇ 1060        ‚îÇ Con optimiz ‚îÇ
‚îÇ Target con Stream   ‚îÇ 280         ‚îÇ 400         ‚îÇ Paralelo    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Frontend: Cliente Ultra-Responsivo

#### Arquitectura del Cliente Web
```javascript
// Clase principal del asistente
class UltraRealAssistant {
    constructor() {
        // Audio configuration
        this.audioConfig = {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            latencyHint: 'interactive'
        };
        
        // Conexiones WebSocket duales
        this.controlWS = null;      // Comandos y transcripts
        this.audioWS = null;        // Streaming de audio binario
        
        // Estado del sistema
        this.isRecording = false;
        this.isPlaying = false;
        this.canInterrupt = false;
        this.sessionStartTime = null;
        this.lastLatency = 0;
        
        // Audio processing
        this.audioContext = null;
        this.mediaRecorder = null;
        this.localStream = null;
        this.audioProcessor = null;
        this.remoteAudio = new Audio();
        
        // UI elementos
        this.statusEl = document.getElementById('status');
        this.transcriptEl = document.getElementById('transcript');
        this.latencyEl = document.getElementById('latency');
        this.waveformCanvas = document.getElementById('waveform');
        this.controlsEl = document.getElementById('controls');
        
        // Buffers para audio processing
        this.audioBuffer = new CircularBuffer(8192);
        this.vadBuffer = new Float32Array(320); // 20ms a 16kHz
        
        this.initialize();
    }
    
    async initialize() {
        await this.setupAudio();
        await this.setupWebSockets();
        this.setupEventListeners();
        this.setupVAD();
        this.startLatencyMonitoring();
    }
    
    async setupAudio() {
        try {
            // Crear AudioContext optimizado para latencia
            this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: 16000,
                latencyHint: 'interactive',
                echoCancellation: true
            });
            
            // Obtener stream del micr√≥fono
            this.localStream = await navigator.mediaDevices.getUserMedia({
                audio: this.audioConfig
            });
            
            // Crear AudioWorklet para processing en tiempo real
            await this.audioContext.audioWorklet.addModule('/audio-processor.js');
            this.audioProcessor = new AudioWorkletNode(this.audioContext, 'audio-processor');
            
            // Conectar audio pipeline
            const source = this.audioContext.createMediaStreamSource(this.localStream);
            source.connect(this.audioProcessor);
            this.audioProcessor.connect(this.audioContext.destination);
            
            // Handler para chunks de audio procesados
            this.audioProcessor.port.onmessage = (event) => {
                if (event.data.type === 'audioChunk' && this.isRecording) {
                    this.sendAudioChunk(event.data.chunk);
                    this.updateWaveform(event.data.chunk);
                    this.processVAD(event.data.chunk);
                }
            };
            
            this.updateStatus('Audio inicializado ‚úì');
            
        } catch (error) {
            console.error('Error inicializando audio:', error);
            this.updateStatus('‚ùå Error: Permisos de micr√≥fono requeridos');
            throw error;
        }
    }
    
    async setupWebSockets() {
        const wsUrl = `${window.location.protocol === 'https:' ? 'wss:' : 'ws:'}//${window.location.host}`;
        const clientId = this.generateClientId();
        
        try {
            // WebSocket para control (JSON messages)
            this.controlWS = new WebSocket(`${wsUrl}/ws/control/${clientId}`);
            
            this.controlWS.onopen = () => {
                console.log('Control WebSocket conectado');
                this.updateStatus('Conectado - Listo para conversar');
            };
            
            this.controlWS.onmessage = (event) => {
                const data = JSON.parse(event.data);
                this.handleControlMessage(data);
            };
            
            this.controlWS.onclose = () => {
                console.log('Control WebSocket desconectado');
                this.updateStatus('‚ùå Desconectado');
            };
            
            // WebSocket para audio (binary data)
            this.audioWS = new WebSocket(`${wsUrl}/ws/audio/${clientId}`);
            this.audioWS.binaryType = 'arraybuffer';
            
            this.audioWS.onopen = () => {
                console.log('Audio WebSocket conectado');
            };
            
            this.audioWS.onmessage = (event) => {
                if (event.data instanceof ArrayBuffer) {
                    this.playAudioResponse(event.data);
                }
            };
            
            this.audioWS.onclose = () => {
                console.log('Audio WebSocket desconectado');
            };
            
            // Esperar a que ambos WebSockets se conecten
            await this.waitForConnections();
            
        } catch (error) {
            console.error('Error conectando WebSockets:', error);
            this.updateStatus('‚ùå Error de conexi√≥n');
            throw error;
        }
    }
    
    setupEventListeners() {
        // Botones de control
        document.getElementById('startBtn').addEventListener('click', () => this.startConversation());
        document.getElementById('stopBtn').addEventListener('click', () => this.stopConversation());
        document.getElementById('interruptBtn').addEventListener('click', () => this.interrupt());
        document.getElementById('resetBtn').addEventListener('click', () => this.resetConversation());
        
        // Atajos de teclado
        document.addEventListener('keydown', (e) => {
            if (e.code === 'Space' && !this.isRecording && !e.repeat) {
                e.preventDefault();
                this.startConversation();
            } else if (e.code === 'Escape') {
                e.preventDefault();
                this.interrupt();
            } else if (e.code === 'KeyR' && e.ctrlKey) {
                e.preventDefault();
                this.resetConversation();
            }
        });
        
        document.addEventListener('keyup', (e) => {
            if (e.code === 'Space' && this.isRecording) {
                e.preventDefault();
                this.stopConversation();
            }
        });
        
        // Detectar p√©rdida de foco para pausar
        document.addEventListener('visibilitychange', () => {
            if (document.hidden && this.isRecording) {
                this.pauseRecording();
            }
        });
    }
    
    setupVAD() {
        // Configuraci√≥n de Voice Activity Detection
        this.vadState = {
            isSpeaking: false,
            silenceStart: null,
            silenceThreshold: 300, // ms
            energyThreshold: 0.01,
            speakingHistory: new Array(10).fill(false)
        };
    }
    
    startLatencyMonitoring() {
        // Monitor de latencia en tiempo real
        setInterval(() => {
            if (this.controlWS && this.controlWS.readyState === WebSocket.OPEN) {
                const ping = {
                    type: 'ping',
                    timestamp: Date.now()
                };
                this.controlWS.send(JSON.stringify(ping));
            }
        }, 5000); // Ping cada 5 segundos
    }
    
    // M√©todos de control de conversaci√≥n
    async startConversation() {
        if (this.isRecording) return;
        
        this.isRecording = true;
        this.sessionStartTime = Date.now();
        
        this.updateStatus('üé§ Escuchando...');
        this.updateControls();
        
        // Enviar se√±al de inicio al servidor
        this.sendControlMessage({
            type: 'start_recording',
            timestamp: this.sessionStartTime
        });
        
        // Iniciar procesamiento de audio
        this.audioProcessor.port.postMessage({ type: 'start' });
    }
    
    stopConversation() {
        if (!this.isRecording) return;
        
        this.isRecording = false;
        this.updateStatus('‚è≥ Procesando...');
        this.updateControls();
        
        // Enviar se√±al de parada
        this.sendControlMessage({
            type: 'stop_recording',
            timestamp: Date.now()
        });
        
        this.audioProcessor.port.postMessage({ type: 'stop' });
    }
    
    interrupt() {
        if (this.isPlaying) {
            // Parar audio de respuesta inmediatamente
            this.remoteAudio.pause();
            this.remoteAudio.currentTime = 0;
            this.isPlaying = false;
        }
        
        // Enviar se√±al de interrupci√≥n
        this.sendControlMessage({
            type: 'interrupt',
            timestamp: Date.now()
        });
        
        this.updateStatus('‚úã Interrumpido - Escuchando...');
        
        // Reiniciar grabaci√≥n autom√°ticamente
        setTimeout(() => this.startConversation(), 100);
    }
    
    resetConversation() {
        this.stopConversation();
        
        this.sendControlMessage({
            type: 'reset_conversation'
        });
        
        // Limpiar UI
        this.transcriptEl.innerHTML = '';
        this.latencyEl.textContent = 'Latencia: -- ms';
        this.updateStatus('üîÑ Conversaci√≥n reiniciada');
    }
    
    // M√©todos de procesamiento de audio
    sendAudioChunk(audioChunk) {
        if (this.audioWS && this.audioWS.readyState === WebSocket.OPEN) {
            // Convertir Float32Array a Int16Array
            const int16Array = new Int16Array(audioChunk.length);
            for (let i = 0; i < audioChunk.length; i++) {
                int16Array[i] = Math.max(-32768, Math.min(32767, audioChunk[i] * 32768));
            }
            
            this.audioWS.send(int16Array.buffer);
        }
    }
    
    playAudioResponse(audioBuffer) {
        try {
            // Crear blob de audio y reproducir inmediatamente
            const blob = new Blob([audioBuffer], { type: 'audio/wav' });
            const audioUrl = URL.createObjectURL(blob);
            
            this.remoteAudio.src = audioUrl;
            this.remoteAudio.play();
            
            this.isPlaying = true;
            this.canInterrupt = true;
            
            this.remoteAudio.onended = () => {
                this.isPlaying = false;
                this.canInterrupt = false;
                URL.revokeObjectURL(audioUrl);
                this.updateStatus('Listo - Haz click o presiona espacio');
            };
            
        } catch (error) {
            console.error('Error reproduciendo audio:', error);
        }
    }
    
    processVAD(audioChunk) {
        // Calcular energ√≠a del audio
        let energy = 0;
        for (let i = 0; i < audioChunk.length; i++) {
            energy += audioChunk[i] * audioChunk[i];
        }
        energy = Math.sqrt(energy / audioChunk.length);
        
        // Determinar si hay speech
        const isSpeaking = energy > this.vadState.energyThreshold;
        
        // Actualizar historial
        this.vadState.speakingHistory.shift();
        this.vadState.speakingHistory.push(isSpeaking);
        
        // L√≥gica de detecci√≥n de fin de turno
        const recentSpeech = this.vadState.speakingHistory.slice(-3).some(s => s);
        
        if (this.vadState.isSpeaking && !recentSpeech) {
            // Comenz√≥ silencio
            if (!this.vadState.silenceStart) {
                this.vadState.silenceStart = Date.now();
            } else if (Date.now() - this.vadState.silenceStart > this.vadState.silenceThreshold) {
                // Silencio suficiente para terminar turno
                this.stopConversation();
                this.vadState.silenceStart = null;
            }
        } else if (recentSpeech) {
            // Hay speech, resetear silencio
            this.vadState.silenceStart = null;
        }
        
        this.vadState.isSpeaking = recentSpeech;
    }
    
    updateWaveform(audioChunk) {
        const canvas = this.waveformCanvas;
        const ctx = canvas.getContext('2d');
        const width = canvas.width = canvas.offsetWidth;
        const height = canvas.height = canvas.offsetHeight;
        
        ctx.clearRect(0, 0, width, height);
        
        // Dibujar waveform
        ctx.strokeStyle = this.isRecording ? '#00ff00' : '#666';
        ctx.lineWidth = 2;
        ctx.beginPath();
        
        const sliceWidth = width / audioChunk.length;
        let x = 0;
        
        for (let i = 0; i < audioChunk.length; i++) {
            const v = (audioChunk[i] + 1) / 2; // Normalizar a 0-1
            const y = v * height;
            
            if (i === 0) {
                ctx.moveTo(x, y);
            } else {
                ctx.lineTo(x, y);
            }
            
            x += sliceWidth;
        }
        
        ctx.stroke();
        
        // Indicador de nivel
        const level = Math.sqrt(audioChunk.reduce((sum, val) => sum + val * val, 0) / audioChunk.length);
        const levelHeight = level * height;
        
        ctx.fillStyle = this.isRecording ? 'rgba(0, 255, 0, 0.3)' : 'rgba(102, 102, 102, 0.3)';
        ctx.fillRect(width - 20, height - levelHeight, 15, levelHeight);
    }
    
    // M√©todos de manejo de mensajes
    handleControlMessage(data) {
        switch (data.type) {
            case 'partial_transcript':
                this.updateTranscript(`üë§ ${data.text}`, 'user partial');
                break;
                
            case 'final_transcript':
                this.updateTranscript(`üë§ ${data.text}`, 'user final');
                this.updateStatus('ü§î Pensando...');
                break;
                
            case 'assistant_thinking':
                this.updateStatus('üß† Generando respuesta...');
                break;
                
            case 'assistant_speaking_start':
                this.updateTranscript(`ü§ñ ${data.text}`, 'assistant speaking');
                this.updateStatus('üó£Ô∏è Hablando...');
                break;
                
            case 'assistant_response_complete':
                this.updateTranscript(`ü§ñ ${data.text}`, 'assistant complete');
                this.calculateLatency(data.timestamp);
                this.updateStatus('Listo - Haz click o presiona espacio');
                break;
                
            case 'language_detected':
                this.updateLanguageIndicator(data.language, data.confidence);
                break;
                
            case 'error':
                this.updateStatus(`‚ùå Error: ${data.message}`);
                this.isRecording = false;
                this.updateControls();
                break;
                
            case 'pong':
                this.updateNetworkLatency(Date.now() - data.timestamp);
                break;
        }
    }
    
    // M√©todos de UI
    updateStatus(status) {
        this.statusEl.textContent = status;
        console.log(`Status: ${status}`);
    }
    
    updateTranscript(text, className) {
        const div = document.createElement('div');
        div.className = `transcript-item ${className}`;
        div.innerHTML = `
            <span class="timestamp">[${new Date().toLocaleTimeString()}]</span>
            <span class="content">${text}</span>
        `;
        
        this.transcriptEl.appendChild(div);
        this.transcriptEl.scrollTop = this.transcriptEl.scrollHeight;
        
        // Limitar historial a √∫ltimas 50 entradas
        while (this.transcriptEl.children.length > 50) {
            this.transcriptEl.removeChild(this.transcriptEl.firstChild);
        }
    }
    
    updateControls() {
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const interruptBtn = document.getElementById('interruptBtn');
        
        startBtn.disabled = this.isRecording;
        stopBtn.disabled = !this.isRecording;
        interruptBtn.disabled = !this.canInterrupt;
        
        if (this.isRecording) {
            startBtn.textContent = 'üé§ Grabando...';
            startBtn.classList.add('recording');
        } else {
            startBtn.textContent = 'üé§ Iniciar';
            startBtn.classList.remove('recording');
        }
    }
    
    calculateLatency(serverTimestamp) {
        if (this.sessionStartTime) {
            const latency = Date.now() - this.sessionStartTime;
            this.lastLatency = latency;
            
            this.latencyEl.textContent = `Latencia: ${latency}ms`;
            
            // Color coding basado en latencia
            if (latency < 300) {
                this.latencyEl.className = 'latency good';
            } else if (latency < 500) {
                this.latencyEl.className = 'latency fair';
            } else {
                this.latencyEl.className = 'latency poor';
            }
        }
    }
    
    updateLanguageIndicator(language, confidence) {
        const indicator = document.getElementById('languageIndicator');
        if (indicator) {
            indicator.textContent = `üåê ${language} (${Math.round(confidence * 100)}%)`;
            indicator.className = confidence > 0.8 ? 'lang-confident' : 'lang-uncertain';
        }
    }
    
    updateNetworkLatency(rtt) {
        const networkEl = document.getElementById('networkLatency');
        if (networkEl) {
            networkEl.textContent = `Red: ${rtt}ms`;
        }
    }
    
    // M√©todos utilitarios
    sendControlMessage(message) {
        if (this.controlWS && this.controlWS.readyState === WebSocket.OPEN) {
            this.controlWS.send(JSON.stringify(message));
        }
    }
    
    generateClientId() {
        return 'client-' + Math.random().toString(36).substr(2, 9);
    }
    
    async waitForConnections() {
        return new Promise((resolve, reject) => {
            const timeout = setTimeout(() => {
                reject(new Error('Connection timeout'));
            }, 10000);
            
            const checkConnections = () => {
                if (this.controlWS.readyState === WebSocket.OPEN && 
                    this.audioWS.readyState === WebSocket.OPEN) {
                    clearTimeout(timeout);
                    resolve();
                } else {
                    setTimeout(checkConnections, 100);
                }
            };
            
            checkConnections();
        });
    }
}

// Clase auxiliar para buffer circular
class CircularBuffer {
    constructor(size) {
        this.size = size;
        this.buffer = new Float32Array(size);
        this.writeIndex = 0;
        this.readIndex = 0;
        this.count = 0;
    }
    
    write(data) {
        for (let i = 0; i < data.length; i++) {
            this.buffer[this.writeIndex] = data[i];
            this.writeIndex = (this.writeIndex + 1) % this.size;
            
            if (this.count < this.size) {
                this.count++;
            } else {
                this.readIndex = (this.readIndex + 1) % this.size;
            }
        }
    }
    
    read(length) {
        const result = new Float32Array(Math.min(length, this.count));
        
        for (let i = 0; i < result.length; i++) {
            result[i] = this.buffer[this.readIndex];
            this.readIndex = (this.readIndex + 1) % this.size;
            this.count--;
        }
        
        return result;
    }
}

// Inicializaci√≥n cuando se carga la p√°gina
document.addEventListener('DOMContentLoaded', () => {
    window.assistant = new UltraRealAssistant();
});
```

#### AudioWorklet Processor (audio-processor.js)
```javascript
class AudioProcessor extends AudioWorkletProcessor {
    constructor() {
        super();
        this.isRecording = false;
        this.chunkSize = 320; // 20ms a 16kHz
        this.buffer = new Float32Array(this.chunkSize);
        this.bufferIndex = 0;
        
        this.port.onmessage = (event) => {
            if (event.data.type === 'start') {
                this.isRecording = true;
            } else if (event.data.type === 'stop') {
                this.isRecording = false;
            }
        };
    }
    
    process(inputs, outputs, parameters) {
        const input = inputs[0];
        
        if (input.length > 0 && this.isRecording) {
            const inputChannel = input[0];
            
            for (let i = 0; i < inputChannel.length; i++) {
                this.buffer[this.bufferIndex] = inputChannel[i];
                this.bufferIndex++;
                
                if (this.bufferIndex >= this.chunkSize) {
                    // Enviar chunk completo
                    this.port.postMessage({
                        type: 'audioChunk',
                        chunk: new Float32Array(this.buffer)
                    });
                    
                    this.bufferIndex = 0;
                }
            }
        }
        
        return true;
    }
}

registerProcessor('audio-processor', AudioProcessor);
```

### 2.3 Backend: Servidor Ultra-Optimizado

#### Arquitectura del Servidor
```python
# main.py - Servidor Ultra-Optimizado para <300ms
import asyncio
import json
import time
import logging
import uuid
from typing import Dict, List, Optional
import numpy as np
from pathlib import Path
import io
import wave

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# APIs Ultra-R√°pidas
from deepgram import (
    DeepgramClient, 
    DeepgramClientOptions, 
    LiveTranscriptionEvents,
    LiveOptions
)
from groq import Groq
from elevenlabs import (
    VoiceSettings, 
    generate, 
    stream, 
    set_api_key, 
    Voice,
    clone
)

# Configuraci√≥n optimizada de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuraci√≥n de la aplicaci√≥n
app = FastAPI(
    title="Asistente Ultra-Real",
    description="Asistente de voz con latencia <300ms y voz clonada",
    version="1.0.0"
)

# CORS para desarrollo
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

# Servir archivos est√°ticos
app.mount("/static", StaticFiles(directory="static"), name="static")
app.mount("/", StaticFiles(directory="static", html=True), name="frontend")

# Configuraci√≥n de APIs
class Config:
    DEEPGRAM_API_KEY = "your-deepgram-key-here"
    GROQ_API_KEY = "your-groq-key-here"
    ELEVENLABS_API_KEY = "your-elevenlabs-key-here"
    CLONED_VOICE_ID = "your-cloned-voice-id"  # Obtenido despu√©s de clonar
    
    # Configuraci√≥n de rendimiento
    MAX_CONCURRENT_CONNECTIONS = 10
    AUDIO_CHUNK_SIZE = 320  # 20ms a 16kHz
    MAX_RECORDING_TIME = 30  # segundos
    RESPONSE_TIMEOUT = 10    # segundos
    
    # Configuraci√≥n de latencia
    DEEPGRAM_ENDPOINTING = 300  # ms
    LLM_MAX_TOKENS = 50         # Respuestas cortas
    TTS_CHUNK_SIZE = 1024       # bytes

config = Config()

# Inicializar clientes de APIs
deepgram_client = DeepgramClient(config.DEEPGRAM_API_KEY)
groq_client = Groq(api_key=config.GROQ_API_KEY)
set_api_key(config.ELEVENLABS_API_KEY)

class ConversationProcessor:
    """Procesador optimizado para conversaciones ultra-r√°pidas"""
    
    def __init__(self, client_id: str):
        self.client_id = client_id
        self.conversation_history: List[Dict] = []
        self.current_session = {
            'start_time': None,
            'language': 'auto',
            'confidence': 0.0,
            'is_processing': False,
            'last_transcript': '',
            'context_tokens': 0
        }
        
        # Configuraci√≥n de personalidad
        self.personality_config = self.load_personality()
        
        # Buffers de audio
        self.audio_buffer = bytearray()
        self.deepgram_connection = None
        
        logger.info(f"Processor inicializado para cliente: {client_id}")
    
    def load_personality(self) -> Dict:
        """Carga configuraci√≥n de personalidad espec√≠fica"""
        return {
            'name': 'Asistente Ultra-Real',
            'style': 'natural_mexican',  # Configurar seg√∫n tu regi√≥n
            'system_prompt': '''
Eres un asistente de voz ultra-natural que responde exactamente como [TU NOMBRE].

PERSONALIDAD:
- Hablas con modismos naturales mexicanos: "√≥rale", "qu√© padre", "est√° padr√≠simo"
- Eres c√°lido, expresivo y aut√©ntico
- Respondes como en conversaci√≥n casual entre amigos
- Usas contracciones y habla natural: "pa'", "que'st√°s", "d'√≥nde"

REGLAS ULTRA-R√ÅPIDAS:
- M√°ximo 15 palabras por respuesta
- Si te interrumpen: "¬øS√≠?", "¬øDime?", "¬øQu√© pas√≥?"
- Mant√©n contexto de interrupciones naturalmente
- Usa expresiones seg√∫n emoci√≥n: entusiasta, explicativo, casual

EJEMPLOS:
- "‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã